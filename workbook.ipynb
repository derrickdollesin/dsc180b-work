{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fe68304d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2651a25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = [\n",
    "    'Attr_ N A M E_ethnicity_signal',\n",
    "    'Attr_ N A M E_gender_signal', \n",
    "    'Var_ A G E',\n",
    "    'Var_ I M M I G R A T I O N', \n",
    "    'Var_ N A M E'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e31f4c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "claude_3_5 = pd.read_csv(r'claude-3.5\\data\\results.csv')['Response']\n",
    "deepseek_chat = pd.read_csv(r'deepseek-chat\\data\\results.csv')['Response']\n",
    "gemini_3 = pd.read_csv(r'gemini-3\\data\\results.csv')['Response']\n",
    "gemma_2_27b = pd.read_csv(r'gemma-2-27b\\data\\results.csv')['Response']\n",
    "gpt_4o = pd.read_csv(r'GPT-4o\\data\\results.csv')['Response']\n",
    "gpt_5 = pd.read_csv(r'gpt-5\\data\\results.csv')['Response']\n",
    "gpt_oss_120b = pd.read_csv(r'gpt-oss-120b\\data\\results.csv')['Response']\n",
    "grok_3_mini = pd.read_csv(r'grok-3-mini\\data\\results.csv')['Response']\n",
    "qwen_max = pd.read_csv(r'qwen-max\\data\\results.csv')['Response']\n",
    "\n",
    "### Need llama 4, nova micro, and \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7018dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = claude_3_5\n",
    "\n",
    "def parse_embedded_json(x):\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "    x = str(x)\n",
    "\n",
    "    # Extract the first JSON object between {...}\n",
    "    m = re.search(r\"\\{.*\\}\", x, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "\n",
    "    json_text = m.group(0)\n",
    "\n",
    "    # Parse JSON safely\n",
    "    try:\n",
    "        return json.loads(json_text)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "parsed = s.apply(parse_embedded_json)\n",
    "\n",
    "# Union of keys across all rows; missing keys => NaN\n",
    "claude_3_5_responses = pd.json_normalize(parsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "307e460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = deepseek_chat\n",
    "\n",
    "def parse_messy_json(x):\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "    \n",
    "    x = str(x)\n",
    "\n",
    "    # 1) Extract the {...} part\n",
    "    m = re.search(r\"\\{.*\\}\", x, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    text = m.group(0)\n",
    "\n",
    "    # 2) First try normal JSON\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # 3) Fix common issue: unquoted keys\n",
    "    # Case 1 Credibility: 4  →  \"Case 1 Credibility\": 4\n",
    "    fixed = re.sub(r'([,{]\\s*)([A-Za-z0-9\\s]+)(\\s*:)', r'\\1\"\\2\"\\3', text)\n",
    "\n",
    "    try:\n",
    "        return json.loads(fixed)\n",
    "    except:\n",
    "        return {}\n",
    "\n",
    "parsed = df.apply(parse_messy_json)\n",
    "\n",
    "scores_df = pd.json_normalize(parsed)\n",
    "\n",
    "deepseek_chat_responses = pd.concat([df.reset_index(drop=True), scores_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "718d305f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gemini_3\n",
    "\n",
    "def parse_embedded_json(x):\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "    x = str(x)\n",
    "\n",
    "    # extract {...} and parse\n",
    "    m = re.search(r\"\\{.*\\}\", x, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    return json.loads(m.group(0))\n",
    "\n",
    "parsed = df.apply(parse_embedded_json)\n",
    "scores_df = pd.json_normalize(parsed)\n",
    "\n",
    "# optional: ensure numeric\n",
    "scores_df = scores_df.apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "gemini_3_responses = pd.concat([df.reset_index(drop=True), scores_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0cca8f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gemma_2_27b\n",
    "\n",
    "def parse_unquoted_keys_block(x: str) -> dict:\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "    x = str(x)\n",
    "\n",
    "    # Grab the {...} block\n",
    "    m = re.search(r\"\\{.*\\}\", x, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    text = m.group(0)\n",
    "\n",
    "    # Quote unquoted keys:  Case 1 Credibility: 4  ->  \"Case 1 Credibility\": 4\n",
    "    # (works at start of object or after commas)\n",
    "    text = re.sub(r'([{\\s,])\\s*([A-Za-z0-9 ]+?)\\s*:', r'\\1\"\\2\":', text)\n",
    "\n",
    "    # Now it should be valid JSON\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "parsed = df.apply(parse_unquoted_keys_block)\n",
    "\n",
    "scores_df = pd.json_normalize(parsed).apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "gemma_2_27b_responses = pd.concat([df.reset_index(drop=True), scores_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3374713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gpt_4o\n",
    "\n",
    "def parse_unquoted_keys_block(x: str) -> dict:\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "    x = str(x)\n",
    "\n",
    "    # Grab the {...} block\n",
    "    m = re.search(r\"\\{.*\\}\", x, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    text = m.group(0)\n",
    "\n",
    "    # Quote unquoted keys:  Case 1 Credibility: 4  ->  \"Case 1 Credibility\": 4\n",
    "    # (works at start of object or after commas)\n",
    "    text = re.sub(r'([{\\s,])\\s*([A-Za-z0-9 ]+?)\\s*:', r'\\1\"\\2\":', text)\n",
    "\n",
    "    # Now it should be valid JSON\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "parsed = df.apply(parse_unquoted_keys_block)\n",
    "\n",
    "scores_df = pd.json_normalize(parsed).apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "gpt_4o_responses = pd.concat([df.reset_index(drop=True), scores_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e3198fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_loose_json(x):\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "\n",
    "    x = str(x).strip()\n",
    "\n",
    "    # Remove possible markdown fences\n",
    "    x = x.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    # Extract the first {...} block\n",
    "    m = re.search(r\"\\{.*\\}\", x, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "    text = m.group(0)\n",
    "\n",
    "    # Try strict JSON first\n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "\n",
    "    # Repair 1: quote unquoted keys\n",
    "    # { Case 1 Credibility: 4, ... } -> { \"Case 1 Credibility\": 4, ... }\n",
    "    repaired = re.sub(r'([{\\s,])([A-Za-z0-9 ]+?)\\s*:', r'\\1\"\\2\":', text)\n",
    "\n",
    "    # Repair 2: remove trailing commas before } or ]\n",
    "    repaired = re.sub(r\",\\s*([}\\]])\", r\"\\1\", repaired)\n",
    "\n",
    "    try:\n",
    "        return json.loads(repaired)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "parsed = gpt_5.apply(parse_loose_json)\n",
    "\n",
    "gpt_5_responses = pd.json_normalize(parsed).apply(pd.to_numeric, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44d2db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_object_from_string(x):\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "\n",
    "    x = str(x)\n",
    "\n",
    "    # Extract the first JSON object in the string\n",
    "    m = re.search(r\"\\{.*\\}\", x, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "\n",
    "    obj_text = m.group(0)\n",
    "\n",
    "    try:\n",
    "        return json.loads(obj_text)\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "parsed = gpt_oss_120b.apply(parse_json_object_from_string)\n",
    "\n",
    "scores_df = pd.json_normalize(parsed).apply(pd.to_numeric, errors=\"coerce\")\n",
    "gpt_oss_120b_responses = pd.concat([gpt_oss_120b.reset_index(drop=True), scores_df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "0c422e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = grok_3_mini   # parse the same Series you're adding columns to\n",
    "\n",
    "def parse_json_obj(x):\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "\n",
    "    x = str(x).strip()\n",
    "\n",
    "    # If there is any stray text, extract the {...} block\n",
    "    m = re.search(r\"\\{.*\\}\", x, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "parsed = s.apply(parse_json_obj)\n",
    "\n",
    "grok_3_mini_responses = pd.json_normalize(parsed).apply(pd.to_numeric, errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "01a3a8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "qwen_max\n",
    "\n",
    "\n",
    "s = qwen_max  # parse the same column you’ll join back to\n",
    "\n",
    "def parse_fenced_json(x):\n",
    "    if pd.isna(x):\n",
    "        return {}\n",
    "\n",
    "    x = str(x)\n",
    "\n",
    "    # remove markdown code fences\n",
    "    x = x.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
    "\n",
    "    # extract {...} in case there’s any extra text\n",
    "    m = re.search(r\"\\{.*\\}\", x, flags=re.DOTALL)\n",
    "    if not m:\n",
    "        return {}\n",
    "\n",
    "    try:\n",
    "        return json.loads(m.group(0))\n",
    "    except json.JSONDecodeError:\n",
    "        return {}\n",
    "\n",
    "parsed = s.apply(parse_fenced_json)\n",
    "\n",
    "qwen_max_responses = pd.json_normalize(parsed).apply(pd.to_numeric, errors=\"coerce\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
